{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS-ML",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NouamaneTazi/NLP/blob/master/topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yEwE5Btq3F0",
        "colab_type": "code",
        "outputId": "66bcc506-8a50-4d77-c470-6d8490ea8701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('brown')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXdfHqFTzhFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import html\n",
        "import string\n",
        "from pprint import pprint\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KU5eXTU0JSL",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing and Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grXls1uCtPWy",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3iSdStvqx-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\n",
        "          \"Hey that's a great deal! I just bought a phone for $199\",\n",
        "          \"@@You'll (learn) a **lot** in the book. Python is an amazing language!@@\"]\n",
        "\n",
        "\n",
        "def tokenize_sentence(text): # tokenize every sentence in list\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences] \n",
        "    return word_tokens\n",
        "\n",
        "#[word_tokenize(text) for text in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFzlPQHy6973",
        "colab_type": "text"
      },
      "source": [
        "## HTML Stripping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1307-qw7sRjV",
        "colab_type": "code",
        "outputId": "c5d56e96-dc2d-4070-a649-6e96a6edc5cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def remove_html(text):\n",
        "  soup= BeautifulSoup(text, 'lxml')\n",
        "  html_free = soup.get_text()\n",
        "  return html_free\n",
        "  \n",
        "remove_html('&pound;682m')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'£682m'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hXF0VUs7BkG",
        "colab_type": "text"
      },
      "source": [
        "## Removing accented characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tJtY7S6KHB1",
        "colab_type": "code",
        "outputId": "46d94576-c9a7-42dd-8969-d6548dc222e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import unicodedata\n",
        "def remove_accented_chars(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "remove_accented_chars('Sómě Áccěntěd těxt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some Accented text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq6HOFTd7Veb",
        "colab_type": "text"
      },
      "source": [
        "## Removing digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXUaCA7YIlQo",
        "colab_type": "code",
        "outputId": "a55d0e8b-4db7-4e6b-9837-cfc2f45f29d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "re.sub(r'\\d+', '', '123hello 456world')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-wz1dXqz9MA",
        "colab_type": "text"
      },
      "source": [
        "## Removing special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtU6qZ2hnZfd",
        "colab_type": "code",
        "outputId": "b484f279-6725-4a87-bead-d3f74fbe547b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "corpus[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The brown fox wasn't that quick and he couldn't win the race\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLR66BsxEfMh",
        "colab_type": "code",
        "outputId": "b5ba4952-fb80-4919-f033-517b405f0ae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    return ' '.join(tokenizer.tokenize(text))\n",
        "\n",
        "remove_special_characters(\"The brown fox wasn't that quick and he couldn't win the race\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown fox wasn t that quick and he couldn t win the race'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0aa-S2O3myt",
        "colab_type": "text"
      },
      "source": [
        "## Expanding contractions: I'd > I would"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXFCT6sYr5O7",
        "colab_type": "code",
        "outputId": "135e7338-0d3f-42d6-f076-cf035e6aba9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from contractions import CONTRACTION_MAP\n",
        "import re\n",
        "\n",
        "def expand_contractions(sentence, contraction_mapping):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
        "    return expanded_sentence\n",
        "    \n",
        "expand_contractions(\"The brown fox wasn't that quick and he couldn't win the race\", CONTRACTION_MAP)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown fox was not that quick and he could not win the race'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE40Kijv4J8W",
        "colab_type": "text"
      },
      "source": [
        "## Removing stopwords (a, the...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0x3Xf-isEmG",
        "colab_type": "code",
        "outputId": "32500131-fd2e-4d61-91e9-84e258f8d8a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def remove_stopwords(expanded_text):\n",
        "    stopword_list = nltk.corpus.stopwords.words('english')\n",
        "    return ' '.join([token for token in word_tokenize(expanded_text) if token not in stopword_list])\n",
        "    \n",
        "remove_stopwords('The brown fox was not that quick and he could not win the race') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown fox quick could win race'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu-9BdY14Pqf",
        "colab_type": "text"
      },
      "source": [
        "## Removing repeated characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eurqcfqssQih",
        "colab_type": "code",
        "outputId": "c4724e6b-7a59-42fb-b059-63b54c6eb563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def remove_repeated_characters(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "    match_substitution = r'\\1\\2\\3'\n",
        "    def replace(old_word):\n",
        "        if wordnet.synsets(old_word):\n",
        "            return old_word\n",
        "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
        "        return replace(new_word) if new_word != old_word else new_word\n",
        "            \n",
        "    correct_tokens = [replace(word) for word in tokens]\n",
        "    return ' '.join(correct_tokens)\n",
        "\n",
        "print (remove_repeated_characters(\"My schooool is realllllyyy amaaazingggg\"))    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My school is really amazing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z5yF-8d4VYC",
        "colab_type": "text"
      },
      "source": [
        "## Stemming (root stem)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1FlKjtlsjlL",
        "colab_type": "code",
        "outputId": "062c9cff-2ac2-4f21-9d0f-b9ce9c5e8dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# porter stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print (ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped'))\n",
        "print (ps.stem('lying'))\n",
        "print (ps.stem('strange'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jump jump jump\n",
            "lie\n",
            "strang\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAcJA2hasoMg",
        "colab_type": "code",
        "outputId": "59159a52-555c-4dd5-f785-f7abf3f17fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# lancaster stemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "ls = LancasterStemmer()\n",
        "print (ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'))\n",
        "print (ls.stem('lying'))\n",
        "print (ls.stem('strange'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jump jump jump\n",
            "lying\n",
            "strange\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWozDFjSssWf",
        "colab_type": "code",
        "outputId": "9a3234d2-5927-4f6f-c440-a62da122ee6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# regex stemmer\n",
        "from nltk.stem import RegexpStemmer\n",
        "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
        "print (rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped'))\n",
        "print (rs.stem('lying'))\n",
        "print (rs.stem('strange'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jump jump jump\n",
            "ly\n",
            "strange\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEPHxR5XszFP",
        "colab_type": "code",
        "outputId": "063d1e4a-c997-4a20-9ea8-0ac218530ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# snowball stemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "ss = SnowballStemmer(\"english\")\n",
        "print ('Supported Languages:', SnowballStemmer.languages)\n",
        "print (ss.stem('jumping'), ss.stem('jumps'), ss.stem('jumped'))\n",
        "print (ss.stem('lying'))\n",
        "print (ss.stem('strange'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Supported Languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
            "jump jump jump\n",
            "lie\n",
            "strang\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJCoZz3Y_C3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "def stem_text(text):\n",
        "    ss = SnowballStemmer(\"english\")\n",
        "    return ' '.join([ss.stem(token) for token in word_tokenize(text)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMZdt11C4Z6S",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization (root word)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjiIED5dqskt",
        "colab_type": "code",
        "outputId": "6f48606a-23c3-438b-b3be-fdcf6fbbf67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# lemmatize nouns\n",
        "print (wnl.lemmatize('cars', 'n'))\n",
        "print (wnl.lemmatize('men', 'n'))\n",
        "\n",
        "# lemmatize verbs\n",
        "print(wnl.lemmatize('running', 'v'))\n",
        "print (wnl.lemmatize('ate', 'v'))\n",
        "\n",
        "# lemmatize adjectives\n",
        "print (wnl.lemmatize('saddest', 'a'))\n",
        "print (wnl.lemmatize('fancier', 'a'))\n",
        "\n",
        "# ineffective lemmatization\n",
        "print (wnl.lemmatize('ate', 'n'))\n",
        "print (wnl.lemmatize('fancier', 'v'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car\n",
            "men\n",
            "run\n",
            "eat\n",
            "sad\n",
            "fancy\n",
            "ate\n",
            "fancier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK4A6bl2CFud",
        "colab_type": "code",
        "outputId": "afbba4a8-b04f-4b41-9048-681159e8adb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"the brown fox was n't that quicker and he could n't win the race lie jump !\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVkpR5YQZhGI",
        "colab_type": "code",
        "outputId": "b75e2ebe-3c09-45bf-d8a0-b1a81c2bc744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# !pip install spacy\n",
        "# import spacy\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load(parse=True, tag=True, entity=True)\n",
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "print(\"Lemmatization :\")\n",
        "print(lemmatize_text(\"The brown fox wasn't that quicker and he couldn't win the race lying jumps!\"))\n",
        "print(\"Stemming :\")\n",
        "print(stem_text(\"The brown fox wasn't that quicker and he couldn't win the race lying jumps!\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemmatization :\n",
            "the brown fox be not that quick and he could not win the race lie jump !\n",
            "Stemming :\n",
            "the brown fox was n't that quicker and he could n't win the race lie jump !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI8nR69271rX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_corpus(corpus, html_stripping=True,\n",
        "contraction_expansion=True, accented_char_removal=True,\n",
        "repeated_char_removal=True, text_lemmatization=True,\n",
        "text_stemming=False, special_char_removal=True,\n",
        "remove_digits=True, stopword_removal=True,\n",
        "stopwords=None, tokenize=False): \n",
        "    normalized_corpus = []  \n",
        "    for text in corpus:\n",
        "        if html_stripping:\n",
        "            text = remove_html(text)\n",
        "        text = text.lower()\n",
        "        if accented_char_removal:\n",
        "            text = remove_accented_chars(text)\n",
        "        if contraction_expansion:\n",
        "            text = expand_contractions(text, CONTRACTION_MAP)            \n",
        "        if special_char_removal:\n",
        "            text = remove_special_characters(text)\n",
        "        if repeated_char_removal:\n",
        "            text = remove_repeated_characters(text) # better remove special chars\n",
        "        if text_lemmatization:\n",
        "            text = lemmatize_text(text) \n",
        "        if text_stemming:\n",
        "            text = stem_text(text)\n",
        "        if remove_digits:\n",
        "            text = re.sub(r'\\d+', '', text)        \n",
        "        if stopword_removal:\n",
        "            text = remove_stopwords(text) # better be expanded and lower_cased before\n",
        "        # remove extra whitespace and newlines\n",
        "        text = re.sub(' +', ' ', text)\n",
        "        text = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', text) \n",
        "        if tokenize:\n",
        "            text = word_tokenize(text)\n",
        "            text = list(filter(None, text))\n",
        "        normalized_corpus.append(text)\n",
        "    return normalized_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOcEleasdhyn",
        "colab_type": "code",
        "outputId": "5eb134b0-5ba1-4088-f99d-df3cb64b68a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sample_text=\"\"\"US unveils world's most powerful supercomputer, beats\n",
        "China. The US has unveiled the world's most powerful supercomputer called\n",
        "'Summit', beating the previous record-holder China's Sunway TaihuLight.\n",
        "With a peak performance of 200,000 trillion calculations per second,\n",
        "it is over twice as fast as Sunway TaihuLight, which is capable of\n",
        "93,000 trillion calculations per second. Summit has 4,608 servers, which\n",
        "reportedly take up the size of two tennis courts.\"\"\"\n",
        "\n",
        "print(normalize_corpus([sample_text],contraction_expansion=False))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['us unveil world powerful supercomputer beat china us unveil world powerful supercomputer call summit beat previous record holder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvqh4VT_mHlS",
        "colab_type": "text"
      },
      "source": [
        "# Keyphrase Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwEiEMsrnDSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from operator import itemgetter\n",
        "# load corpus\n",
        "alice = gutenberg.sents(fileids='carroll-alice.txt')\n",
        "alice = [' '.join(ts) for ts in alice]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNFUEFfFiCNX",
        "colab_type": "code",
        "outputId": "97953a86-5a66-4088-8e1d-63edaee81981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "norm_alice = list(filter(None, normalize_corpus(alice, text_lemmatization=False)))\n",
        "# print and compare first line\n",
        "print(alice[0], '\\n', norm_alice[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Alice ' s Adventures in Wonderland by Lewis Carroll 1865 ] \n",
            " alice adventures wonderland lewis carroll\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBjf3AnTipR-",
        "colab_type": "text"
      },
      "source": [
        "## Compute n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSo1lX3XjTs0",
        "colab_type": "code",
        "outputId": "d6f5388e-902a-40e1-804b-94ecfe02779c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_documents([item.split() for item in norm_alice])\n",
        "finder.nbest(bigram_measures.raw_freq, 10) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('said', 'alice'),\n",
              " ('mock', 'turtle'),\n",
              " ('march', 'hare'),\n",
              " ('said', 'king'),\n",
              " ('thought', 'alice'),\n",
              " ('said', 'hatter'),\n",
              " ('white', 'rabbit'),\n",
              " ('said', 'mock'),\n",
              " ('said', 'caterpillar'),\n",
              " ('said', 'gryphon')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj638ZuYkOl5",
        "colab_type": "code",
        "outputId": "e7595532-5e5b-4a6e-cd11-fcaa94b6107a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "finder.nbest(bigram_measures.pmi, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('abide', 'figures'),\n",
              " ('acceptance', 'elegant'),\n",
              " ('accounting', 'tastes'),\n",
              " ('accustomed', 'usurpation'),\n",
              " ('act', 'crawling'),\n",
              " ('adjourn', 'immediate'),\n",
              " ('adoption', 'energetic'),\n",
              " ('affair', 'trusts'),\n",
              " ('agony', 'terror'),\n",
              " ('alarmed', 'proposal')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzE8X--v0LWp",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "  ### Bag of Words Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLzUUC6l0aXP",
        "colab_type": "code",
        "outputId": "a6cf5e77-afe1-4167-9ee9-9d80c2e9256a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# get bag of words features in sparse format\n",
        "cv = CountVectorizer(min_df=0., max_df=1.)\n",
        "cv_matrix = cv.fit_transform(norm_corpus)\n",
        "cv_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d65df0f28cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# get bag of words features in sparse format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcv_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcv_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'norm_corpus' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujkGTVYdz0nU",
        "colab_type": "text"
      },
      "source": [
        "# Topic modeling methods\n",
        " ## LDA Analysis\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c4pzhEKz0D9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}